'''
    Simulate the running time and delay of different algorithms in small network (100 nodes and 2000 edges), which is generated by network_parameters.py.
'''
import time
import subprocess
import sys
import random
from network_parameters import generate_network_parameters 
import statistics
import os
import shutil
import csv
from pathlib import Path

def generate_new_parameters(num_nodes, num_edges):
    yaml_file_path = generate_network_parameters(num_nodes, num_edges)
    yaml_copy_path = yaml_file_path.replace('.yaml', '_copy.yaml')
    shutil.copy2(yaml_file_path, yaml_copy_path)
    return yaml_file_path

def run_script(script_path, yaml_file_path):
    start_time = time.time()
    result = subprocess.run([sys.executable, script_path, yaml_file_path], capture_output=True, text=True)
    end_time = time.time()
    print(f"Algorithm output:\n{result.stdout}")
    return result.stdout, end_time - start_time

def parse_output(output):
    delay = None
    running_time = None
    
    for line in output.split('\n'):
        if line.startswith("END_TO_END_DELAY:"):
            try:
                delay = float(line.split(":")[-1].strip())
            except ValueError:
                print(f"Unable to parse end-to-end delay: {line}")
        
        if line.startswith("RUNNING_TIME:"):
            try:
                running_time = float(line.split(":")[-1].strip())
            except ValueError:
                print(f"Unable to parse running time: {line}")
    
    if delay is None:
        print("End-to-end delay not found")
    if running_time is None:
        print("Running time not found")
    
    if delay is None or running_time is None:
        print("Complete output:")
        print(output)
    
    return delay, running_time

def save_data_to_csv(results, avg_runtimes, avg_delay_diffs):
    algorithms = list(results.keys())  # Including all algorithms, including NLP
    with open('algorithm_comparison.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Algorithm', 'Average Running Time', 'Average Delay Difference from NLP'])
        
        # First write NLP data
        nlp_runtime = statistics.mean(results['NLP']['run_times'])
        writer.writerow(['NLP', nlp_runtime, 0])  # NLP's delay difference is 0
        
        # Write other algorithms' data
        for alg, runtime, delay_diff in zip(algorithms[1:], avg_runtimes, avg_delay_diffs):
            writer.writerow([alg, runtime, delay_diff])


def main():
    num_runs = 500  # Number of runs for each scale
    network_sizes = []
    avg_runtimes = []
    avg_delay_diffs = []
    
    # Get current file directory as base path
    BASE_PATH = Path(__file__).parent
    print(f"Current base path: {BASE_PATH}")  # Debug info

    algorithms = {
        "MINLP": BASE_PATH / "MINLP.py",
        # "LP":    BASE_PATH / "LP_in_CPEG.py",
        "CPEG":  BASE_PATH / "CPEG algorithm.py",
        "CNE":   BASE_PATH / "CNE_algorithm.py",
        "CCN":   BASE_PATH / "CCN.py",
        "MPCN":  BASE_PATH / "MPCN.py"
    }

    # Verify all algorithm files exist
    for name, path in algorithms.items():
        if not path.exists():
            print(f"Warning: Algorithm file not found {name}: {path}")
        else:
            print(f"Found algorithm file {name}: {path}")

    results = {name: {"run_times": [], "delays": []} for name in algorithms}

    for i in range(num_runs):
        print(f"\nExecute the {i+1}th test round...")
        num_nodes = 100
        num_edges = 2000
        network_sizes.append((num_nodes, num_edges))
        
        # Generate YAML file and ensure it's in the correct location
        yaml_file_path = generate_new_parameters(num_nodes, num_edges)
        print(f"Generated YAML file path: {yaml_file_path}")  # Debug info
        
        for name, path in algorithms.items():
            print(f"   Run {name} algorithm...")
            try:
                # Use the str representation of the Path object
                output, _ = run_script(str(path), yaml_file_path)
                delay, running_time = parse_output(output)
                if delay is not None and running_time is not None:
                    results[name]["run_times"].append(running_time)
                    results[name]["delays"].append(delay)
                else:
                    print(f"    {name} algorithm did not return valid delay or running time")
            except Exception as e:
                print(f"    {name} algorithm failed: {str(e)}")
                import traceback
                print(traceback.format_exc())  # Print the complete error stack

    print("\nFinal comparison results:")
    nlp_delays = results["NLP"]["delays"]
    for name, data in results.items():
        avg_run_time = statistics.mean(data["run_times"]) if data["run_times"] else 0
        print(f"{name}:")
        print(f"   Average running time = {avg_run_time:.4f} seconds")
        print(f"   Valid runs: {len(data['delays'])}/{num_runs}")
        
        if name != "NLP":
            delay_diffs = []
            for d, n in zip(data["delays"], nlp_delays):
                if d is not None and n is not None:
                    delay_diffs.append(d - n)
            if delay_diffs:
                avg_delay_diff = statistics.mean(delay_diffs)
                print(f"   Average delay difference from NLP = {avg_delay_diff:.4f}")
                print(f"   Valid comparisons: {len(delay_diffs)}")
                avg_runtimes.append(avg_run_time)
                avg_delay_diffs.append(avg_delay_diff)
            else:
                print("   No valid delay difference comparisons")
        else:
            print("   As the benchmark delay")

    # Save data to CSV file
    save_data_to_csv(results, avg_runtimes, avg_delay_diffs)
    
if __name__ == "__main__":
    main()


